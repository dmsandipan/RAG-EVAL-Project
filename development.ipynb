{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b21fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Desktop\\repos\\rag_evaluation\\rag_evaluation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from rag_eval.rag.rag import *\n",
    "from rag_eval.utils.client import llm\n",
    "from rag_eval.metrics.retrieval_metrics import contextual_precision_llm, evaluate_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragbench_hotpotqa = load_dataset(\"rungalileo/ragbench\", \"hotpotqa\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f6020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_point(ragbench_dict):\n",
    "    \n",
    "    # prepare dictionary \n",
    "    context_dict = {}\n",
    "    for document in ragbench_dict[\"documents_sentences\"]:\n",
    "        for sentence in document:\n",
    "            context_dict[sentence[0]] = sentence[1] \n",
    "\n",
    "    reference_sentences = []\n",
    "\n",
    "    for key in ragbench_dict[\"all_relevant_sentence_keys\"]:\n",
    "        reference_sentences.append(context_dict[key])\n",
    "    reference = ragbench_dict[\"response\"]\n",
    "\n",
    "    output = {\n",
    "        \"question\": ragbench_dict[\"question\"],\n",
    "        \"context\":context_dict.values(),\n",
    "        \"reference_context\": reference_sentences,\n",
    "        \"reference\": reference,\n",
    "        \"context_dict\" : context_dict\n",
    "    }\n",
    "    return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f6da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "\n",
    "for item in range(10):\n",
    "    test_set.append(prepare_data_point(ragbench_hotpotqa[item]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_instance(test_instance):\n",
    "    OPENAI_API_KEY = \"\"\n",
    "\n",
    "    rag_model = rag(OPENAI_API_KEY)\n",
    "    rag_model.process_docs_to_vectorstore(list(test_instance[\"context\"]))\n",
    "    docs = rag_model.retrieve_docs(test_instance[\"question\"])\n",
    "    docs = [item[0].page_content for item in docs]\n",
    "    response = rag_model.query(test_instance[\"question\"])\n",
    "    metrics = evaluate_output(query = test_instance[\"question\"],\n",
    "                response = response,\n",
    "                retrieval_list= docs ,\n",
    "                ground_truth= test_instance[\"reference\"],\n",
    "                reference_list= test_instance[\"reference_context\"])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb45a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [evaluate_instance(item) for item in test_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b78a9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'contextual_precision': (1.0,\n",
       "   {'classification': [0.5, 1, 0.5, 0, 0],\n",
       "    'explanation': [\"The first chunk confirms that the school is named after John Rankin Rogers, which is relevant to the school’s name as implied in the query, but does not mention the school district. So, it's partially relevant.\",\n",
       "     'The second chunk directly states that Governor John R. Rogers High School is in the Puyallup School District of Washington, which fully supports the answer.',\n",
       "     \"The third chunk provides biographical information about John Rankin Rogers, connecting to the namesake context, but does not mention anything about the school district or the school's location, so it’s only partially relevant due to the name linkage.\",\n",
       "     \"The fourth chunk refers to a Rogers High School in Texas, not related to the school in the query or answer, so it's not relevant.\",\n",
       "     \"The fifth chunk expands on the Texas Rogers High School and its district, not concerning the Washington school or its district, so it's not relevant.\"]}),\n",
       "  'contextual_recall': (1.0,\n",
       "   {'claim': ['Governor John R. Rogers High School is named after John Rankin Rogers',\n",
       "     'Governor John R. Rogers High School is located in the Puyallup School District of Washington, United States'],\n",
       "    'classification': [1, 1],\n",
       "    'explanation': ['The retrieved chunk: \\'Commonly referred to as \"Rogers\" or \"RHS,\" the high school is named after former Washington State governor John Rankin Rogers.\\' directly states that Governor John R. Rogers High School is named after John Rankin Rogers.',\n",
       "     \"The retrieved chunk: 'Governor John R. Rogers High School is a high school in the Puyallup School District of Washington, United States.' directly confirms the school's location in the Puyallup School District of Washington, United States.\"]}),\n",
       "  'contextual_relevancy': 0.6366155171209795,\n",
       "  'mmr': 0.5},\n",
       " {'contextual_precision': (1.0,\n",
       "   {'classification': [1, 0.5, 1, 0, 0],\n",
       "    'explanation': ['This chunk directly states that Daniel Ricciardo won the 44-lap race for Red Bull Racing after starting from fifth position, which matches the query and the generated answer.',\n",
       "     \"This chunk discusses Ricciardo's replacement of Mark Webber and his association with Red Bull Racing, which is topically relevant but not directly about winning the race in question.\",\n",
       "     \"This chunk confirms Daniel Ricciardo is an Australian racing driver who competes for Red Bull Racing, directly supporting the answer's identification of Ricciardo as the driver.\",\n",
       "     \"This chunk is about a different racing series and period in Ricciardo's career, unrelated to the Formula One race in question and does not help answer the query.\",\n",
       "     \"This chunk discusses Ricciardo's involvement with Red Bull Racing but does not mention anything about a 44-lap race win or him winning for Red Bull Racing, so it is not relevant to the given answer.\"]}),\n",
       "  'contextual_recall': (1.0,\n",
       "   {'claim': ['Daniel Ricciardo won the 44-lap race.',\n",
       "     'Daniel Ricciardo won the race for the Red Bull Racing team.',\n",
       "     'Daniel Ricciardo is an Australian racing driver.'],\n",
       "    'classification': [1, 1, 1],\n",
       "    'explanation': [\"The first retrieved chunk states, 'The 44-lap race was won by Daniel Ricciardo for the Red Bull Racing team,' which directly supports this claim.\",\n",
       "     \"The same chunk also specifies 'for the Red Bull Racing team', directly supporting this claim.\",\n",
       "     \"A retrieved chunk says, 'Daniel Joseph Ricciardo... is an Australian racing driver who is currently competing in Formula One for Red Bull Racing,' which directly supports this claim.\"]}),\n",
       "  'contextual_relevancy': 0.5511202178347133,\n",
       "  'mmr': 1.0},\n",
       " {'contextual_precision': (1.0,\n",
       "   {'classification': [1, 0.5, 1, 0, 0],\n",
       "    'explanation': ['This chunk confirms that Nick Offerman is widely known for his role in Parks and Recreation, directly connecting the star to the series as mentioned in the query and answer.',\n",
       "     \"This chunk provides information about the film 'November' but does not connect it to any cast member directly. It is topically related because it identifies 'November' as a film, but does not reference Offerman or Parks and Recreation.\",\n",
       "     \"This chunk confirms Nick Offerman was a co-star in 'November', directly supporting the generated answer that he appeared in the film.\",\n",
       "     \"This chunk provides details about the cast of 'November', but does not mention Offerman, Parks and Recreation, or connect the star to the film. Not relevant.\",\n",
       "     \"This chunk discusses Nick Offerman's post-Parks and Recreation roles (e.g., in Fargo and Ice Age), but not about 'November', so it is not used for the answer.\"]}),\n",
       "  'contextual_recall': (1.0,\n",
       "   {'claim': ['Nick Offerman appeared in the film November',\n",
       "     'Nick Offerman played the role of Ron Swanson in Parks and Recreation'],\n",
       "    'classification': [1, 1],\n",
       "    'explanation': [\"There is a retrieved chunk: 'The film co-stars Michael Ealy, Nora Dunn, Anne Archer, Nick Offerman, and Matthew Carey.' This confirms Nick Offerman appeared in the film November.\",\n",
       "     'There is a retrieved chunk: \\'Nicholas \"Nick\" Offerman ... is an American actor ... widely known for his breakout role as Ron Swanson in the acclaimed NBC sitcom \"Parks and Recreation\".\\' This confirms that Nick Offerman played the role of Ron Swanson in Parks and Recreation.']}),\n",
       "  'contextual_relevancy': 0.40642872029659083,\n",
       "  'mmr': 1.0},\n",
       " {'contextual_precision': (1.0,\n",
       "   {'classification': [0.5, 1, 1, 0, 0],\n",
       "    'explanation': ['The first chunk gives basic genus-level information about Crocosmia but does not indicate its distribution or environment. It is related to the topic but does not directly contribute to the answer.',\n",
       "     'The second chunk provides direct information about Crocosmia being native to eastern South Africa, Lesotho, and Swaziland, supporting the claim that it is found further south.',\n",
       "     'The third chunk directly states that Cimicifuga is native to temperate regions of the Northern Hemisphere, supporting the comparative statement in the answer.',\n",
       "     'The fourth chunk talks about the taxonomic reclassification of Cimicifuga but does not provide geographic or environmental information relevant to the query or answer.',\n",
       "     'The fifth chunk only says that it is a popular ornamental plant, offering no relevant information to the specific geographic distribution asked in the query.']}),\n",
       "  'contextual_recall': (1.0,\n",
       "   {'claim': ['Crocosmia is found in an environment further south than Cimicifuga.'],\n",
       "    'classification': [1],\n",
       "    'explanation': [\"One chunk specifies Crocosmia is native to eastern South Africa, Lesotho, and Swaziland (all Southern Hemisphere locations), supporting that Crocosmia is found further south. Another chunk indicates Cimicifuga is native to temperate regions of the Northern Hemisphere, reinforcing that Crocosmia's native environment is further south than Cimicifuga's.\"]}),\n",
       "  'contextual_relevancy': 0.5281490282595883,\n",
       "  'mmr': 0},\n",
       " {'contextual_precision': (0.3333333333333333,\n",
       "   {'classification': [0, 0, 1, 0, 0],\n",
       "    'explanation': ['This chunk provides background about The Dingoes but contains no information regarding the shooting of Chris Stockley or the shooter’s fate, so it is not relevant to the query or answer.',\n",
       "     \"This chunk regards the band's debut single and chart performance, unrelated to the shooting or shooter’s death.\",\n",
       "     \"This chunk details that Chris Stockley was shot and by whom (Dennis Allen), directly supporting the answer by giving context for who the shooter was, even though the year of the shooter's death is not mentioned. Hence, it is useful and relevant for answering why the system cannot provide the year.\",\n",
       "     \"This chunk discusses the band's albums, not the shooting or Dennis Allen.\",\n",
       "     'This chunk concerns someone named Chris being paralyzed in a plane crash in 1993, but it does not link to the events involving The Dingoes, Chris Stockley, or Dennis Allen.']}),\n",
       "  'contextual_recall': (0.5,\n",
       "   {'claim': ['Dennis Bruce Allen was the man who shot Chris Stockley of The Dingoes.',\n",
       "     'Dennis Bruce Allen died in 1987.'],\n",
       "    'classification': [1, 0],\n",
       "    'explanation': [\"There is a retrieved chunk stating that Chris Stockley was shot in the stomach by Melbourne drug-dealer, Dennis Allen, which supports that Dennis Allen was the man who shot Chris Stockley (though the chunk does not specify the middle name 'Bruce', the reference is clear).\",\n",
       "     'None of the retrieved chunks provide information about the year Dennis Allen (or Dennis Bruce Allen) died, so the claim that he died in 1987 is unsupported.']}),\n",
       "  'contextual_relevancy': 0.449609451450567,\n",
       "  'mmr': 0},\n",
       " {'contextual_precision': (1.0,\n",
       "   {'classification': [1, 1, 1, 0.5, 0.5],\n",
       "    'explanation': [\"Chunk 1 establishes The Four Seasons' international success in the 1960s/1970s, directly supporting the claim about their popularity overlapping with the Beatles.\",\n",
       "     \"Chunk 2 supports the generated answer by stating that The Four Seasons were the most popular rock band before the Beatles, directly addressing their popularity relative to the Beatles' rise.\",\n",
       "     'Chunk 3 confirms that Local H released an album in 2014, demonstrating they were not around when the Beatles were active, supporting the answer about Local H.',\n",
       "     'Chunk 4 only adds an alternate name for the Four Seasons after 1970; this is tangentially relevant, reflecting their ongoing existence but not essential to the answer about the Beatles period.',\n",
       "     \"Chunk 5 discusses the band's lineup/history in 1960, showing when the Four Seasons formed but is somewhat tangential—helpful in establishing timing, but not as directly tied to popularity.\"]}),\n",
       "  'contextual_recall': (0.3333333333333333,\n",
       "   {'claim': ['Both American bands The Four Seasons and Local H were popular at the same time as the Beatles.',\n",
       "     'The Four Seasons were noted as the most popular rock band before the Beatles.',\n",
       "     \"Local H was formed in the late 1980s, so their popularity would have overlapped with the Beatles' era.\"],\n",
       "    'classification': [0, 1, 0],\n",
       "    'explanation': [\"There is no retrieved chunk that contains or supports the claim that both bands were popular at the same time as the Beatles or that Local H existed during the Beatles' era. The timeframes for Local H do not match with the Beatles based on the provided chunks.\",\n",
       "     \"One chunk directly states: 'The Vocal Group Hall of Fame has stated that the group was the most popular rock band before the Beatles.', which supports this claim.\",\n",
       "     'The only mention of Local H is about a 2014 EP release, with no chunks about its formation date or popularity period. Therefore, there is no evidence in the chunks that supports this claim.']}),\n",
       "  'contextual_relevancy': 0.46067972516819433,\n",
       "  'mmr': 1.0},\n",
       " {'contextual_precision': (0.8666666666666667,\n",
       "   {'classification': [0.5, 0.5, 0, 0, 1],\n",
       "    'explanation': ['Chunk 1 mentions relationship to the first Viscount Barnewall, which is topically relevant as it establishes lineage, but does not directly mention supporting the deposed king.',\n",
       "     \"Chunk 2 talks about being part of the Barnewall family and mentions descendants who held the viscountcy; it's somewhat related to identifying the lineage but does not address support for the king.\",\n",
       "     \"Chunk 3 discusses the third Viscount, son of George Barnewall, and doesn't link to the query about supporting the king or the relevant viscount.\",\n",
       "     \"Chunk 4 provides family background but doesn't connect to the query about the king or the Viscount Barnewall's supporter.\",\n",
       "     'Chunk 5 directly states the grandson (the third Viscount) was a supporter of James II (the king deposed in 1688), exactly as described in the answer, so it is fully relevant.']}),\n",
       "  'contextual_recall': (0.5,\n",
       "   {'claim': ['The king deposed in the Glorious Revolution of 1688 was supported by the grandson of the third Viscount Barnewall.'],\n",
       "    'classification': [0.5],\n",
       "    'explanation': [\"There is mention in the retrieved context that someone is the son of the Honourable George Barnewall, younger son of the third Viscount, and that 'His grandson, the third Viscount, was a supporter of James II and outlawed.' However, no retrieved chunk clearly and directly states that the king deposed in the Glorious Revolution (James II) was supported by the grandson of the third Viscount Barnewall, only that the third Viscount (or his grandson) was a supporter of James II. The relevant relationship is partially supported, but the full logic connecting all the entities is not explicitly present.\"]}),\n",
       "  'contextual_relevancy': 0.5800975990654116,\n",
       "  'mmr': 0},\n",
       " {'contextual_precision': (0.4777777777777777,\n",
       "   {'classification': [0, 0, 0.5, 0.5, 1],\n",
       "    'explanation': ['The first chunk mentions elementary flying training schools and the RAAF, but makes no reference to No. 3 EFTS or Benalla. It does not contribute to answering which city center is closest or where No. 3 operated.',\n",
       "     'The second chunk repeats the first and also lacks any reference to No. 3 EFTS, the airport location, or the city center, thus does not contribute to the answer.',\n",
       "     'The third chunk establishes No. 3 EFTS as a WW2 RAAF training unit, making it tangentially relevant and providing topic background, but does not specifically mention its location or relevance to Benalla.',\n",
       "     \"The fourth chunk names 'No. 3 Elementary Flying Training School' without location context. It gives a partial reference (the correct unit), so it's partially relevant but insufficient alone for the full answer.\",\n",
       "     \"The fifth chunk directly mentions Benalla, Victoria, linked to an elementary flying school (although it specifies No. 11, not No. 3, it still points to Benalla as a relevant airport location). Because Benalla is key in the answer, this chunk is directly useful, even if number mismatch exists, since it establishes Benalla as an airport with elementary flying schools, supporting the answer's main fact about the closest city center.\"]}),\n",
       "  'contextual_recall': (0.0,\n",
       "   {'claim': ['No. 3 Elementary Flying Training School RAAF operated at Essendon Airport.',\n",
       "     \"Essendon Airport is the closest to Melbourne's City Centre in Australia.\"],\n",
       "    'classification': [0, 0],\n",
       "    'explanation': [\"None of the retrieved chunks mention Essendon Airport as the location of No. 3 Elementary Flying Training School RAAF, nor do they provide information about its proximity to Melbourne's City Centre.\",\n",
       "     \"There is no information in the retrieved context about Essendon Airport or its proximity to Melbourne's City Centre, or any mention that it is the nearest airport to it.\"]}),\n",
       "  'contextual_relevancy': 0.5848927388136451,\n",
       "  'mmr': 0},\n",
       " {'contextual_precision': (1.0,\n",
       "   {'classification': [1, 0.5, 1, 0, 0],\n",
       "    'explanation': [\"The first chunk directly states the Hedley Mascot Mine's location in Hedley, British Columbia, Canada, which supports the answer.\",\n",
       "     \"The second chunk provides additional context about the Hedley Mascot Mine but focuses on its operations and physical location relative to Hedley rather than its precise location; it's related but not directly used in the answer.\",\n",
       "     \"The third chunk directly states the location of Catcha Lake in the central part of Nova Scotia's Halifax Regional Municipality, supporting the answer.\",\n",
       "     'The fourth chunk discusses other mining complexes not related to either Catcha Lake or Hedley Mascot Mine, so it is not relevant.',\n",
       "     \"The fifth chunk provides general information about an unspecified mine's opening and construction, which is not relevant to the query or answer.\"]}),\n",
       "  'contextual_recall': (1.0,\n",
       "   {'claim': [\"Catcha Lake is located in the central part of Nova Scotia's Halifax Regional Municipality in Canada.\",\n",
       "     'The Hedley Mascot Mine is located in Hedley, British Columbia, Canada.'],\n",
       "    'classification': [1, 1],\n",
       "    'explanation': [\"There is a retrieved chunk stating 'Catcha Lake is a Canadian lake located in the central part of Nova Scotia's Halifax Regional Municipality.' This supports the full claim, as Nova Scotia is in Canada.\",\n",
       "     \"There is a retrieved chunk stating 'The Hedley Mascot Mine was a gold mine in Hedley, British Columbia, Canada.' This directly supports the claim made in the ground truth answer.\"]}),\n",
       "  'contextual_relevancy': 0.43789759122092686,\n",
       "  'mmr': 1.0},\n",
       " {'contextual_precision': (0.95,\n",
       "   {'classification': [0.5, 1, 1, 0, 1],\n",
       "    'explanation': [\"Chunk 1 provides biographical context for Ulli Lommel but does not mention his time in the USA or help answer the query directly—it's only topically related.\",\n",
       "     \"Chunk 2 confirms Benjamin Stoloff's birthplace in the USA, directly supporting the answer's statement that he was born there.\",\n",
       "     \"Chunk 3 gives Benjamin Stoloff's birth and death years, showing he could have lived his whole life in the USA, contributing significantly to the answer.\",\n",
       "     \"Chunk 4 discusses Lommel's artistic associations but does not reference his residence or time in the USA, so it is not used in the answer.\",\n",
       "     'Chunk 5 directly states when Ulli Lommel began living and working in the USA, which is essential to determine the length of time he lived in the US and used in the answer.']}),\n",
       "  'contextual_recall': (0.8333333333333334,\n",
       "   {'claim': ['Ulli Lommel has lived longer in the USA than Benjamin Stoloff.',\n",
       "     'Benjamin Stoloff died in 1960.',\n",
       "     'Ulli Lommel has lived and worked in the USA since 1977.'],\n",
       "    'classification': [0.5, 1, 1],\n",
       "    'explanation': ['There is no chunk that directly compares the duration both individuals spent in the USA or explicitly states that Ulli Lommel has lived in the USA longer than Benjamin Stoloff. However, relevant dates of residence and death for each person are retrieved, so the claim can be partially inferred from the retrieved facts.',\n",
       "     'Chunk: \\'Benjamin \"Ben\" Stoloff (October 6, 1895 – September 8, 1960) was an American film director and producer.\\' Directly confirms that Benjamin Stoloff died in 1960.',\n",
       "     \"Chunk: 'Since 1977 he has lived and worked in the USA, where he has written, directed and starred in over 50 movies.' This directly supports that Ulli Lommel has lived and worked in the USA since 1977.\"]}),\n",
       "  'contextual_relevancy': 0.4742131807860459,\n",
       "  'mmr': 0}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a = outputs\n",
    "output_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb48725",
   "metadata": {},
   "source": [
    "### ares context_relevance scoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29100c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_eval.utils.client import llm\n",
    "output = []\n",
    "output_by_doc = []\n",
    "docs_by_docs = []\n",
    "for item in test_set:\n",
    "    OPENAI_API_KEY = \"\"\n",
    "\n",
    "    rag_model = rag(OPENAI_API_KEY)\n",
    "    rag_model.process_docs_to_vectorstore(list(item[\"context\"]))\n",
    "    docs = rag_model.retrieve_docs(item[\"question\"])\n",
    "    docs = [item[0].page_content for item in docs]\n",
    "    response = rag_model.query(item[\"question\"])\n",
    "    score = 0\n",
    "    doc_outputs = []\n",
    "    for chunk in docs:\n",
    "\n",
    "        prompt = (\n",
    "        f\"\"\"You are an expert dialogue agent. \n",
    "        Your task is to analyze the provided document and determine whether it is relevant for responding to the dialogue. \n",
    "        In your evaluation, you should consider the content of the document and how it relates to the provided dialogue. \n",
    "        'Output your final verdict by strictly following this format: [[Yes]]\" if the document is relevant and \"[[No]]\" if the document provided is not relevant. \n",
    "        \"Do not provide any additional explanation for your decision.\\n\\n\n",
    "        \n",
    "        Question: {item['question']}\n",
    "        Document: {chunk}\n",
    "        \"\"\"\n",
    "        )\n",
    "        model = llm()\n",
    "        res = model.query(prompt)\n",
    "\n",
    "        if \"Yes\" in res:\n",
    "            score +=1\n",
    "        doc_outputs.append(res)\n",
    "    \n",
    "    output_by_doc.append(doc_outputs)\n",
    "    docs_by_docs.append(docs)\n",
    "    if score <1:\n",
    "        output.append(score)\n",
    "    else:\n",
    "        output.append(score/len(docs))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e908831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Commonly referred to as \"Rogers\" or \"RHS,\" the high school is named after former Washington State governor John Rankin Rogers.',\n",
       "  'Governor John R. Rogers High School is a high school in the Puyallup School District of Washington, United States.',\n",
       "  'John Rankin Rogers (September 4, 1838 – December 26, 1901) was the third Governor of the state of Washington.',\n",
       "  'Rogers High School is a 3A public high school located in Rogers, Texas (USA).',\n",
       "  'It is part of the Rogers Independent School District located in southeastern Bell County.'],\n",
       " ['The 44-lap race was won by Daniel Ricciardo for the Red Bull Racing team, after starting from fifth position.',\n",
       "  'After Mark Webber announced his retirement from Formula One, Ricciardo was confirmed as his replacement at Red Bull Racing for 2014.',\n",
       "  'Daniel Joseph Ricciardo ( ; born 1 July 1989) is an Australian racing driver who is currently competing in Formula One for Red Bull Racing.',\n",
       "  'In 2005 he switched to the Formula Renault 3.5 with Swiss team Jenzer Motorsport, despite three DNS in the first 3 and missing one race of that season he finished 13th with 34 points with a best finish of 3rd in the second race in the Bugatti Circuit, to 2006 he switched to Carlin where he finished 6th in the first race of the season in Zolder, however he failed to qualify to the Second Race of the weekend and he finished 4th in Circuit de Monaco and 8th in both races in Istanbul Park, after this round he asked to leave Carlin and the Red Bull Junior Team, Red Bull officially released him and he was replaced by fellow Red Bull Junior Team member and Future Formula One Champion Sebastian Vettel, after leaving Red Bull, he returned to the United States to complete in the Atlantic Championship mid-season and he finished 20th with 45 points and since then has not raced anymore in a Major Series, until he decided to retire.',\n",
       "  'He has been test and third driver for the Jordan and Red Bull Racing Formula One teams, as well as driving for Minardi and Red Bull Racing in 2005 and 2006.'],\n",
       " ['Nicholas \"Nick\" Offerman (born June 26, 1970) is an American actor, voice actor, producer, writer, comedian and carpenter widely known for his breakout role as Ron Swanson in the acclaimed NBC sitcom \"Parks and Recreation\", for which he received the Television Critics Association Award for Individual Achievement in Comedy.',\n",
       "  'November is a 2004 American psychological thriller film first screened at the 2004 Sundance Film Festival.',\n",
       "  'The film co-stars Michael Ealy, Nora Dunn, Anne Archer, Nick Offerman, and Matthew Carey.',\n",
       "  'It stars Courteney Cox as Sophie, a photographer whose life begins to unravel following a traumatic incident on November 7 that involved her boyfriend, played by James LeGros.',\n",
       "  'His first major television role since the end of \"Parks and Recreation\" was his role as Karl Weathers in the FX series \"Fargo\", for which he received a Critics\\' Choice Television Award nomination for Best Supporting Actor in a Movie/Miniseries and Gavin in the Ice Age franchise.'],\n",
       " ['Crocosmia ( ; J. E. Planchon, 1851) (montbretia) is a small genus of flowering plants in the iris family, Iridaceae.',\n",
       "  'Crocosmia paniculata (Aunt Eliza) is a bulbous flowering plant that is native to eastern South Africa, Lesotho, and Swaziland, growing in wet areas by streams, marshes, and drainages.',\n",
       "  'Cimicifuga (bugbane or cohosh) was a genus of between 12-18 species of flowering plants belonging to the family Ranunculaceae, native to temperate regions of the Northern Hemisphere.',\n",
       "  'Like some other species in genus \"Actaea\", this plant was formerly included in the genus \"Cimicifuga\".',\n",
       "  'It is a popular ornamental plant.'],\n",
       " ['The Dingoes is an Australian country rock band initially active from 1973 to 1979, formed in Melbourne which relocated to the United States from 1976.',\n",
       "  'The Dingoes\\' debut single, \"Way Out West\", was released in November 1973, and peaked in the top 40 of the Australian Kent Music Report singles chart.',\n",
       "  'Mal Logan (who provided keyboards the first LP) on keyboards joined after Stockley was hospitalised when shot in the stomach by Melbourne drug-dealer, Dennis Allen, who was attempting to gate crash a party.',\n",
       "  'They had three top 40 albums, \"The Dingoes\" in 1974, \"Fives Times the Sun\" in 1977, and \"Orphans of the Storm\" in 1979.',\n",
       "  'Initially, Chris was a good man who ran his business fairly, but was left permanently resentful of his life when he was maimed in a plane crash in 1993, which left him paralyzed from the waist down.'],\n",
       " ['The Four Seasons is an American rock and pop band that became internationally successful in the 1960s and 1970s.',\n",
       "  'The Vocal Group Hall of Fame has stated that the group was the most popular rock band before the Beatles.',\n",
       "  \"Local H's Awesome Mix Tape #2 is an extended play by American alternative rock duo Local H, which was released in December 2014 through their merchandiser, G&P Records.\",\n",
       "  'Since 1970, they have also been known at times as Frankie Valli and the Four Seasons.',\n",
       "  'In 1960, the group known as the Four Lovers evolved into the Four Seasons, with Frankie Valli as the lead singer, Bob Gaudio (formerly of the Royal Teens) on keyboards and tenor vocals, Tommy DeVito on lead guitar and baritone vocals, and Nick Massi on electric bass and bass vocals.'],\n",
       " ['He was the great-grandson of the Honourable Richard Barnewall, younger son of the first Viscount.',\n",
       "  'He belonged to a junior branch of the family of Lord Trimlestown: his own descendants held the title Viscount Barnewall of Kingsland.',\n",
       "  'He was the son of the Honourable George Barnewall, younger son of the third Viscount.',\n",
       "  \"The Kingsland Barnewalls were a junior branch of the family of Baron Trimleston; Nicholas's great-grandfather Sir Patrick Barnewall had achieved political prominence through his friendship with Thomas Cromwell and done well out of the Dissolution of the Monasteries .\",\n",
       "  'His grandson, the third Viscount, was a supporter of James II and outlawed.'],\n",
       " [\"It was one of twelve elementary flying training schools employed by the RAAF to provide introductory flight instruction to new pilots as part of Australia's contribution to the Empire Air Training Scheme. No.\",\n",
       "  \"It was one of twelve elementary flying training schools employed by the RAAF to provide introductory flight instruction to new pilots as part of Australia's contribution to the Empire Air Training Scheme. No.\",\n",
       "  '3 EFTS) was a Royal Australian Air Force (RAAF) pilot training unit that operated during World War II.',\n",
       "  'No. 3 Elementary Flying Training School (No.',\n",
       "  '11 Elementary Flying School at Benalla, Victoria.'],\n",
       " ['The Hedley Mascot Mine was a gold mine in Hedley, British Columbia, Canada.',\n",
       "  'The Hedley Mascot Mine operated between 1936 and 1949 and was one of the most unusual mining operations in the world, being built entirely on the side of a mountain, 5,000 feet above the town of Hedley or seven thousand feet above sea level.',\n",
       "  \"Catcha Lake is a Canadian lake located in the central part of Nova Scotia's Halifax Regional Municipality.\",\n",
       "  'In addition to the main underground mine, the mining complex also includes MUK-96 underground mine, Raspadskaya Koksovaya underground mine, and Razrez Raspadsky open-pit mine, as also the Raspadskaya preparation plant.',\n",
       "  'The mine was opened in 1973 and its construction was completed in 1977.'],\n",
       " ['Ulli Lommel (born 21 December 1944) is a German actor and director, noted for his many collaborations with Rainer Werner Fassbinder and his association with the New German Cinema movement.',\n",
       "  'Stoloff was born in Philadelphia, Pennsylvania.',\n",
       "  'Benjamin \"Ben\" Stoloff (October 6, 1895 – September 8, 1960) was an American film director and producer.',\n",
       "  'Lommel is also well known for the time which he spent at The Factory and as a creative associate of Andy Warhol, with whom he made several films and works of art.',\n",
       "  'Since 1977 he has lived and worked in the USA, where he has written, directed and starred in over 50 movies.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_by_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f85aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[[No]]', '[[Yes]]', '[[No]]', '[[No]]', '[[No]]'],\n",
       " ['[[Yes]]', '[[No]]', '[[Yes]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[No]]', '[[Yes]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[Yes]]', '[[No]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[No]]', '[[No]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[No]]', '[[No]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[No]]', '[[No]]', '[[No]]', '[[Yes]]'],\n",
       " ['[[No]]', '[[No]]', '[[No]]', '[[No]]', '[[No]]'],\n",
       " ['[[Yes]]', '[[No]]', '[[Yes]]', '[[No]]', '[[No]]'],\n",
       " ['[[No]]', '[[No]]', '[[No]]', '[[No]]', '[[No]]']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_by_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdbfb0",
   "metadata": {},
   "source": [
    "### Ragas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc44e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_eval.metrics.retrieval_metrics import ragas_contextual_precision_llm, ragas_contextual_recall_llm\n",
    "\n",
    "output_precision = []\n",
    "output_recall = []\n",
    "for item in test_set:\n",
    "    OPENAI_API_KEY = \"\"\n",
    "    rag_model = rag(OPENAI_API_KEY)\n",
    "    rag_model.process_docs_to_vectorstore(list(item[\"context\"]))\n",
    "    docs = rag_model.retrieve_docs(item[\"question\"])\n",
    "    docs = [item[0].page_content for item in docs]\n",
    "    response = rag_model.query(item[\"question\"])\n",
    "\n",
    "    output_precision.append(ragas_contextual_precision_llm(question = item[\"question\"], retrieved_context = docs, reference = item[\"reference\"]))\n",
    "    output_recall.append(ragas_contextual_recall_llm(question= item[\"question\"], response=response, reference= item[\"reference\"], retrieved_context= docs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66866f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49999999995,\n",
       " 0.8333333332916666,\n",
       " 0.3333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.36666666664833336,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e0ee7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b03984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.DataFrame(output_a)\n",
    "output_df[\"ares_contextual_relevance_docs\"] = docs_by_docs\n",
    "output_df[\"ares_contextual_relevance_scores\"] = output_by_doc\n",
    "output_df[\"ragas_contextual_precision\"] = output_precision\n",
    "output_df[\"ragas_contextual_recall\"] = output_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b72f8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_precision</th>\n",
       "      <th>contextual_recall</th>\n",
       "      <th>contextual_relevancy</th>\n",
       "      <th>mmr</th>\n",
       "      <th>ares_contextual_relevance_docs</th>\n",
       "      <th>ares_contextual_relevance_scores</th>\n",
       "      <th>ragas_contextual_precision</th>\n",
       "      <th>ragas_contextual_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1.0, {'classification': [0.5, 1, 0.5, 0, 0], ...</td>\n",
       "      <td>(1.0, {'claim': ['Governor John R. Rogers High...</td>\n",
       "      <td>0.636616</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[Commonly referred to as \"Rogers\" or \"RHS,\" th...</td>\n",
       "      <td>[[[No]], [[Yes]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...</td>\n",
       "      <td>(1.0, {'claim': ['Daniel Ricciardo won the 44-...</td>\n",
       "      <td>0.551120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The 44-lap race was won by Daniel Ricciardo f...</td>\n",
       "      <td>[[[Yes]], [[No]], [[Yes]], [[No]], [[No]]]</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...</td>\n",
       "      <td>(1.0, {'claim': ['Nick Offerman appeared in th...</td>\n",
       "      <td>0.406429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Nicholas \"Nick\" Offerman (born June 26, 1970)...</td>\n",
       "      <td>[[[No]], [[No]], [[Yes]], [[No]], [[No]]]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1.0, {'classification': [0.5, 1, 1, 0, 0], 'e...</td>\n",
       "      <td>(1.0, {'claim': ['Crocosmia is found in an env...</td>\n",
       "      <td>0.528149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Crocosmia ( ; J. E. Planchon, 1851) (montbret...</td>\n",
       "      <td>[[[No]], [[Yes]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.3333333333333333, {'classification': [0, 0,...</td>\n",
       "      <td>(0.5, {'claim': ['Dennis Bruce Allen was the m...</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[The Dingoes is an Australian country rock ban...</td>\n",
       "      <td>[[[No]], [[No]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(1.0, {'classification': [1, 1, 1, 0.5, 0.5], ...</td>\n",
       "      <td>(0.3333333333333333, {'claim': ['Both American...</td>\n",
       "      <td>0.460680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The Four Seasons is an American rock and pop ...</td>\n",
       "      <td>[[[No]], [[No]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.8666666666666667, {'classification': [0.5, ...</td>\n",
       "      <td>(0.5, {'claim': ['The king deposed in the Glor...</td>\n",
       "      <td>0.580098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[He was the great-grandson of the Honourable R...</td>\n",
       "      <td>[[[No]], [[No]], [[No]], [[No]], [[Yes]]]</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.4777777777777777, {'classification': [0, 0,...</td>\n",
       "      <td>(0.0, {'claim': ['No. 3 Elementary Flying Trai...</td>\n",
       "      <td>0.584893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[It was one of twelve elementary flying traini...</td>\n",
       "      <td>[[[No]], [[No]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...</td>\n",
       "      <td>(1.0, {'claim': ['Catcha Lake is located in th...</td>\n",
       "      <td>0.437898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[The Hedley Mascot Mine was a gold mine in Hed...</td>\n",
       "      <td>[[[Yes]], [[No]], [[Yes]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.95, {'classification': [0.5, 1, 1, 0, 1], '...</td>\n",
       "      <td>(0.8333333333333334, {'claim': ['Ulli Lommel h...</td>\n",
       "      <td>0.474213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Ulli Lommel (born 21 December 1944) is a Germ...</td>\n",
       "      <td>[[[No]], [[No]], [[No]], [[No]], [[No]]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                contextual_precision  \\\n",
       "0  (1.0, {'classification': [0.5, 1, 0.5, 0, 0], ...   \n",
       "1  (1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...   \n",
       "2  (1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...   \n",
       "3  (1.0, {'classification': [0.5, 1, 1, 0, 0], 'e...   \n",
       "4  (0.3333333333333333, {'classification': [0, 0,...   \n",
       "5  (1.0, {'classification': [1, 1, 1, 0.5, 0.5], ...   \n",
       "6  (0.8666666666666667, {'classification': [0.5, ...   \n",
       "7  (0.4777777777777777, {'classification': [0, 0,...   \n",
       "8  (1.0, {'classification': [1, 0.5, 1, 0, 0], 'e...   \n",
       "9  (0.95, {'classification': [0.5, 1, 1, 0, 1], '...   \n",
       "\n",
       "                                   contextual_recall  contextual_relevancy  \\\n",
       "0  (1.0, {'claim': ['Governor John R. Rogers High...              0.636616   \n",
       "1  (1.0, {'claim': ['Daniel Ricciardo won the 44-...              0.551120   \n",
       "2  (1.0, {'claim': ['Nick Offerman appeared in th...              0.406429   \n",
       "3  (1.0, {'claim': ['Crocosmia is found in an env...              0.528149   \n",
       "4  (0.5, {'claim': ['Dennis Bruce Allen was the m...              0.449609   \n",
       "5  (0.3333333333333333, {'claim': ['Both American...              0.460680   \n",
       "6  (0.5, {'claim': ['The king deposed in the Glor...              0.580098   \n",
       "7  (0.0, {'claim': ['No. 3 Elementary Flying Trai...              0.584893   \n",
       "8  (1.0, {'claim': ['Catcha Lake is located in th...              0.437898   \n",
       "9  (0.8333333333333334, {'claim': ['Ulli Lommel h...              0.474213   \n",
       "\n",
       "   mmr                     ares_contextual_relevance_docs  \\\n",
       "0  0.5  [Commonly referred to as \"Rogers\" or \"RHS,\" th...   \n",
       "1  1.0  [The 44-lap race was won by Daniel Ricciardo f...   \n",
       "2  1.0  [Nicholas \"Nick\" Offerman (born June 26, 1970)...   \n",
       "3  0.0  [Crocosmia ( ; J. E. Planchon, 1851) (montbret...   \n",
       "4  0.0  [The Dingoes is an Australian country rock ban...   \n",
       "5  1.0  [The Four Seasons is an American rock and pop ...   \n",
       "6  0.0  [He was the great-grandson of the Honourable R...   \n",
       "7  0.0  [It was one of twelve elementary flying traini...   \n",
       "8  1.0  [The Hedley Mascot Mine was a gold mine in Hed...   \n",
       "9  0.0  [Ulli Lommel (born 21 December 1944) is a Germ...   \n",
       "\n",
       "             ares_contextual_relevance_scores  ragas_contextual_precision  \\\n",
       "0   [[[No]], [[Yes]], [[No]], [[No]], [[No]]]                    0.500000   \n",
       "1  [[[Yes]], [[No]], [[Yes]], [[No]], [[No]]]                    0.833333   \n",
       "2   [[[No]], [[No]], [[Yes]], [[No]], [[No]]]                    0.333333   \n",
       "3   [[[No]], [[Yes]], [[No]], [[No]], [[No]]]                    0.000000   \n",
       "4    [[[No]], [[No]], [[No]], [[No]], [[No]]]                    0.000000   \n",
       "5    [[[No]], [[No]], [[No]], [[No]], [[No]]]                    0.000000   \n",
       "6   [[[No]], [[No]], [[No]], [[No]], [[Yes]]]                    0.366667   \n",
       "7    [[[No]], [[No]], [[No]], [[No]], [[No]]]                    0.000000   \n",
       "8  [[[Yes]], [[No]], [[Yes]], [[No]], [[No]]]                    0.000000   \n",
       "9    [[[No]], [[No]], [[No]], [[No]], [[No]]]                    0.000000   \n",
       "\n",
       "   ragas_contextual_recall  \n",
       "0                 1.000000  \n",
       "1                 1.000000  \n",
       "2                 1.000000  \n",
       "3                 1.000000  \n",
       "4                 0.000000  \n",
       "5                 0.333333  \n",
       "6                 1.000000  \n",
       "7                 0.000000  \n",
       "8                 1.000000  \n",
       "9                 0.500000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a065c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
